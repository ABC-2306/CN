# Install required packages
!pip install transformers faiss-cpu sentence-transformers torch

import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# Sample document corpus
documents = [
    "The Eiffel Tower is located in Paris, France and was built in 1889.",
    "Python is a high-level programming language known for its simplicity.",
    "Machine learning is a subset of artificial intelligence that uses algorithms.",
    "The Amazon rainforest is the largest tropical rainforest in the world.",
    "Photosynthesis is the process by which plants convert sunlight into energy.",
    "The Internet was invented in the late 1960s as ARPANET.",
    "DNA contains the genetic instructions for all living organisms.",
    "The Great Wall of China is over 13,000 miles long."
]

# Initialize embedding model
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

# Create embeddings and build FAISS index
embeddings = embedding_model.encode(documents)
dimension = embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(embeddings.astype('float32'))

print(f"Indexed {index.ntotal} documents")

# Load LLM for generation
tokenizer = AutoTokenizer.from_pretrained("distilgpt2")
tokenizer.pad_token = tokenizer.eos_token
llm = AutoModelForCausalLM.from_pretrained("distilgpt2")
device = "cuda" if torch.cuda.is_available() else "cpu"
llm.to(device)

# RAG function
def retrieve_and_generate(query, k=2):
    # Retrieve relevant documents
    query_embedding = embedding_model.encode([query])
    distances, indices = index.search(query_embedding.astype('float32'), k)

    retrieved_docs = [documents[idx] for idx in indices[0]]
    context = " ".join(retrieved_docs)

    print(f"\nQuery: {query}")
    print(f"Retrieved Documents: {retrieved_docs}")

    # Generate response with context
    prompt = f"Context: {context}\n\nQuestion: {query}\n\nAnswer:"
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=256).to(device)

    outputs = llm.generate(
        **inputs,
        max_length=inputs['input_ids'].shape[1] + 100,
        temperature=0.7,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id,
        num_return_sequences=1
    )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    answer = response.split("Answer:")[-1].strip()

    print(f"Generated Answer: {answer}\n")
    return answer

# Test queries
queries = [
    "Where is the Eiffel Tower?",
    "What is machine learning?",
    "Tell me about photosynthesis"
]

for query in queries:
    retrieve_and_generate(query)

# Add new documents dynamically
new_docs = ["Tokyo is the capital city of Japan with a population over 13 million."]
new_embeddings = embedding_model.encode(new_docs)
index.add(new_embeddings.astype('float32'))
documents.extend(new_docs)

print(f"Total indexed documents: {index.ntotal}")
